{"version":3,"sources":["../src/types/llm.ts"],"names":[],"mappings":"AAGA,MAAM,CAAN,IAAY,SAKX;AALD,WAAY,SAAS;IACnB,8BAAiB,CAAA;IACjB,0BAAa,CAAA;IACb,8BAAiB,CAAA;IACjB,4CAA+B,CAAA;AACjC,CAAC,EALW,SAAS,KAAT,SAAS,QAKpB;AAGD,MAAM,CAAN,IAAY,KAYX;AAZD,WAAY,KAAK;IACf,iCAAwB,CAAA;IACxB,2CAAkC,CAAA;IAClC,uBAAc,CAAA;IACd,kCAAyB,CAAA;IACzB,qCAA4B,CAAA;IAC5B,wCAA+B,CAAA;IAC/B,uCAA8B,CAAA;IAC9B,iDAAwC,CAAA;IACxC,wCAA+B,CAAA;IAC/B,sCAA6B,CAAA;IAC7B,0CAAiC,CAAA;AACnC,CAAC,EAZW,KAAK,KAAL,KAAK,QAYhB","file":"llm.js","sourcesContent":["import type { AtomName, BaseContext } from './atom';\n\n/** LLM Model Type */\nexport enum ModelType {\n  GPT3_5 = 'gpt3.5',\n  GPT4 = 'gpt4',\n  doubao = 'doubao',\n  CHART_ADVISOR = 'chart-advisor'\n}\n\n/** Specific LLM MODEL */\nexport enum Model {\n  GPT3_5 = 'gpt-3.5-turbo',\n  GPT3_5_1106 = 'gpt-3.5-turbo-1106',\n  GPT4 = 'gpt-4',\n  GPT_4_0613 = 'gpt-4-0613',\n  GPT_4o = 'gpt-4o-2024-08-06',\n  DOUBAO_LITE = 'doubao-lite-32K',\n  DOUBAO_PRO = 'doubao-pro-128k',\n  DOUBAO_PRO_32K = 'doubao-pro-32k-240828',\n  CHART_ADVISOR = 'chart-advisor',\n  DEEPSEEK_V3 = 'deepseek-chat',\n  DEEPSEEK_R1 = 'deepseek-reasoner'\n}\n\n/** Custrom Request function callback of llm */\nexport type RequestFunc = (\n  messages: LLMMessage[],\n  tools: ToolMessage[] | undefined,\n  options: ILLMOptions | undefined\n) => Promise<LLMResponse>;\n\n/** Base LLM Options */\ninterface BaseLLMOptions {\n  /** URL of your LLM service. For gpt, default is openAI API. */\n  url?: string;\n  /** llm request header, which has higher priority */\n  headers?: HeadersInit;\n  /** post or get */\n  method?: 'POST' | 'GET';\n  /** LLM Model */\n  model?: Model | string;\n  /** Max token in LLM Chart */\n  maxTokens?: number;\n  /** Temperature of LLM */\n  temperature?: number;\n  /** show llm thoughs or not */\n  showThoughts?: boolean;\n  /** repetition penalty */\n  frequencyPenalty?: number;\n  /** topP */\n  topP?: number;\n  /** function call */\n  functionCall?: 'auto' | 'none' | { name: string };\n}\n\n/** LLM Options */\nexport interface ILLMOptions extends BaseLLMOptions {\n  /** customRequest */\n  customRequestFunc?: {\n    [key in AtomName]?: RequestFunc;\n  };\n}\n\n/** VMind Options */\nexport interface VMindOptions extends BaseLLMOptions {\n  customRequestFunc?: {\n    chartAdvisor?: RequestFunc;\n    dataQuery?: RequestFunc;\n    dataExtraction?: RequestFunc;\n    chartCommand?: RequestFunc;\n    IntelligentInsight?: RequestFunc;\n  };\n}\n\n/** Tool Messages of tool result */\nexport interface ToolCall {\n  id: string;\n  type: 'function';\n  function: {\n    name: string;\n    arguments: string;\n  };\n}\n\n/** LLM Messages api */\nexport interface LLMMessage {\n  /** prompt role, system or user query or tool result */\n  role: 'system' | 'user' | 'assistant' | 'tool';\n  content: string;\n  name?: string;\n  tool_calls?: ToolCall[];\n  tool_call_id?: string;\n}\n\nexport type ParamType = 'function' | 'object' | 'array' | 'string' | 'number' | 'boolean';\nexport interface JsonSchemaParams {\n  type: ParamType;\n  description?: string;\n  enum?: string[];\n  properties?: JsonSchemaParams;\n  required?: string[];\n  items?: JsonSchemaParams;\n  minItems?: number;\n  uniqueItems?: boolean;\n  $ref?: string;\n}\n\nexport interface ToolMessage {\n  type: 'function';\n  function: {\n    name: string;\n    description: string;\n    parameters?: {\n      type: string;\n      properties: Record<string, JsonSchemaParams>;\n      strict?: boolean;\n      required?: string[];\n      addionalProperties?: boolean;\n    };\n  };\n}\n\n/** LLM Response API */\nexport interface LLMResponse extends BaseContext {\n  choices?: {\n    index: number;\n    message: any;\n    finish_reason?: string;\n  }[];\n  error?: string;\n  [key: string]: any;\n}\n\nexport interface MemoryOptions {\n  /** max history messages saved */\n  maxMessagesCnt?: number;\n}\n"]}