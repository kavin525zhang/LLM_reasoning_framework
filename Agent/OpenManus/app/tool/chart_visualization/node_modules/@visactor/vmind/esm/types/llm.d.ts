import type { AtomName, BaseContext } from './atom';
export declare enum ModelType {
    GPT3_5 = "gpt3.5",
    GPT4 = "gpt4",
    doubao = "doubao",
    CHART_ADVISOR = "chart-advisor"
}
export declare enum Model {
    GPT3_5 = "gpt-3.5-turbo",
    GPT3_5_1106 = "gpt-3.5-turbo-1106",
    GPT4 = "gpt-4",
    GPT_4_0613 = "gpt-4-0613",
    GPT_4o = "gpt-4o-2024-08-06",
    DOUBAO_LITE = "doubao-lite-32K",
    DOUBAO_PRO = "doubao-pro-128k",
    DOUBAO_PRO_32K = "doubao-pro-32k-240828",
    CHART_ADVISOR = "chart-advisor",
    DEEPSEEK_V3 = "deepseek-chat",
    DEEPSEEK_R1 = "deepseek-reasoner"
}
export type RequestFunc = (messages: LLMMessage[], tools: ToolMessage[] | undefined, options: ILLMOptions | undefined) => Promise<LLMResponse>;
interface BaseLLMOptions {
    url?: string;
    headers?: HeadersInit;
    method?: 'POST' | 'GET';
    model?: Model | string;
    maxTokens?: number;
    temperature?: number;
    showThoughts?: boolean;
    frequencyPenalty?: number;
    topP?: number;
    functionCall?: 'auto' | 'none' | {
        name: string;
    };
}
export interface ILLMOptions extends BaseLLMOptions {
    customRequestFunc?: {
        [key in AtomName]?: RequestFunc;
    };
}
export interface VMindOptions extends BaseLLMOptions {
    customRequestFunc?: {
        chartAdvisor?: RequestFunc;
        dataQuery?: RequestFunc;
        dataExtraction?: RequestFunc;
        chartCommand?: RequestFunc;
        IntelligentInsight?: RequestFunc;
    };
}
export interface ToolCall {
    id: string;
    type: 'function';
    function: {
        name: string;
        arguments: string;
    };
}
export interface LLMMessage {
    role: 'system' | 'user' | 'assistant' | 'tool';
    content: string;
    name?: string;
    tool_calls?: ToolCall[];
    tool_call_id?: string;
}
export type ParamType = 'function' | 'object' | 'array' | 'string' | 'number' | 'boolean';
export interface JsonSchemaParams {
    type: ParamType;
    description?: string;
    enum?: string[];
    properties?: JsonSchemaParams;
    required?: string[];
    items?: JsonSchemaParams;
    minItems?: number;
    uniqueItems?: boolean;
    $ref?: string;
}
export interface ToolMessage {
    type: 'function';
    function: {
        name: string;
        description: string;
        parameters?: {
            type: string;
            properties: Record<string, JsonSchemaParams>;
            strict?: boolean;
            required?: string[];
            addionalProperties?: boolean;
        };
    };
}
export interface LLMResponse extends BaseContext {
    choices?: {
        index: number;
        message: any;
        finish_reason?: string;
    }[];
    error?: string;
    [key: string]: any;
}
export interface MemoryOptions {
    maxMessagesCnt?: number;
}
export {};
