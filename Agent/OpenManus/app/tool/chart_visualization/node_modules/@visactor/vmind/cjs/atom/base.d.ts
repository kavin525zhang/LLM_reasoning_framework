import type { BaseContext } from '../types/atom';
import { AtomName } from '../types/atom';
import type { LLMMessage, LLMResponse, ToolMessage } from '../types/llm';
import type { BaseOptions } from './type';
export declare class BaseAtom<Ctx extends BaseContext, O extends BaseOptions> {
    name: AtomName;
    protected originContext: Ctx;
    protected context: Ctx;
    protected responses: LLMMessage[];
    options: O;
    isLLMAtom: boolean;
    history: {
        map: Map<number, Ctx>;
        idList: number[];
        id: number;
    };
    constructor(context: Partial<Ctx>, options: Partial<O>);
    protected setNewContext(context: Ctx): void;
    undo(id?: string): void;
    redo(id?: string): void;
    buildDefaultContext(context: Ctx): Ctx;
    buildDefaultOptions(): O;
    updateContext(context: Partial<Ctx>, replace?: boolean): Ctx;
    updateOptions(options: Partial<O>): void;
    reset(context?: Partial<Ctx>): void;
    getContext(): Ctx;
    getContextBeforeRun(): Ctx;
    shouldRunByContextUpdate(context: Ctx): boolean;
    run(userInput?: {
        context?: Ctx;
        query?: string;
        messages?: LLMMessage[];
    }): Promise<Ctx>;
    protected runBeforeLLM(): Ctx;
    protected runWithLLMError(error: string): Ctx;
    runWithChat(query: string): Promise<Ctx>;
    protected _runWithOutLLM(): Ctx;
    protected getHistoryLLMMessages(query?: string): LLMMessage[];
    protected getLLMMessages(query?: string): LLMMessage[];
    protected getFunctionCalls(): ToolMessage[];
    protected parseLLMContent(resJson: any, toolJson?: any, llmRes?: LLMResponse): Ctx;
    protected recordLLMResponse(data: LLMResponse, query?: string): void;
    setResponses(messages: LLMMessage[]): void;
    getResponses(): LLMMessage[];
    clearHistory(): void;
}
